# Timeout Fix for Backend Processing

## Problem

**Error:** 
```
Failed to proxy http://127.0.0.1:8000/api/py/text/process Error: socket hang up
code: 'ECONNRESET'
```

**Root Cause:** Next.js dev proxy has a ~30-second timeout, but our backend processing takes longer.

**Why it takes time:**
- **Concept extraction**: 1 call per chunk (1-3 chunks typical)
- **Embedding generation**: 1 batch call for all concepts
- **Relationship extraction**: 1 call per batch of 60 concepts
- **Total time**: 30-120 seconds for complex texts

The backend was working fine, but **Next.js proxy was killing the connection** before the backend could respond.

## Fixes Applied

**üîë KEY FIX:** Frontend now bypasses Next.js proxy in development mode (calls backend directly at `http://127.0.0.1:8000`). This avoids the proxy's hardcoded 30-second timeout.

### 1. **Backend Optimization** (`api/services/text_processing.py`)

**Skip chunking for short texts:**
```python
# Optimization: Skip chunking for texts < 3000 chars
if len(text) < 3000:
    chunks = [text]  # Single chunk = faster processing
else:
    chunks = self._chunk(text)  # Multi-chunk for long texts
```

**Progress logging:**
```python
print(f"Processing {len(chunks)} chunk(s) for {len(text)} characters")
print(f"  Extracting concepts from chunk 1/1...")
print(f"  Found 15 concepts")
print(f"  Generating embeddings and merging duplicates...")
print(f"  Concepts after deduplication: 12")
print(f"  Extracting relationships for 12 concepts...")
print(f"  Found 24 relationships")
```

This lets you monitor progress in the backend console.

### 2. **Bypass Next.js Proxy** (`app/page.tsx` and `app/components/ChatPanel.tsx`)

**The Key Fix:** Call backend directly in development to avoid Next.js proxy timeout.

```typescript
// Use direct backend URL to bypass Next.js proxy timeout
const apiUrl = process.env.NODE_ENV === 'development' 
  ? 'http://127.0.0.1:8000/api/py/text/process'  // Direct backend in dev
  : '/api/py/text/process';  // Proxy in production

const response = await fetch(apiUrl, { /* ... */ });
```

**Why this works:**
- Next.js proxy has hardcoded ~30s timeout
- Direct fetch() to backend respects our custom timeouts
- Production still uses proxy (which works with serverless functions)

### 3. **Frontend Timeout Handler** (`app/page.tsx`)

**Added 3-minute timeout with abort controller:**
```typescript
// Create abort controller for timeout handling
const controller = new AbortController();
const timeoutId = setTimeout(() => controller.abort(), 180000); // 3 min

const response = await fetch('/api/py/text/process', {
  method: 'POST',
  // ... other options
  signal: controller.signal,  // Attach abort signal
});

clearTimeout(timeoutId);  // Clear timeout if request completes
```

**Handle timeout errors gracefully:**
```typescript
catch (err: any) {
  if (err.name === 'AbortError') {
    setError('Request timed out. Try: 1) Shorter text, 2) Higher min_importance (0.3-0.5), or 3) Wait and retry.');
  } else {
    setError(err.message || 'An error occurred...');
  }
}
```

### 4. **Backend Startup Script** (`start_backend.py`)

**New script with extended timeouts:**
```python
uvicorn.run(
    "api.index:app",
    host="127.0.0.1",
    port=8000,
    reload=True,
    timeout_keep_alive=300,  # 5 minutes keep-alive
    timeout_graceful_shutdown=30,
    log_level="info"
)
```

## How to Use

### Start Backend with Extended Timeouts

**Option 1: Use the new start script (RECOMMENDED)**
```bash
python start_backend.py
```

**Option 2: Use uvicorn directly with timeout flags**
```bash
uvicorn api.index:app --reload --port 8000 --timeout-keep-alive 300
```

**Option 3: Old way (may still timeout)**
```bash
cd api
uvicorn index:app --reload
```

### Start Frontend

```bash
npm run dev
```

## Expected Processing Times

| Text Length | Concepts Expected | Processing Time |
|-------------|-------------------|-----------------|
| 100-500 chars | 5-10 | 5-15 seconds |
| 500-1500 chars | 10-25 | 15-30 seconds |
| 1500-3000 chars | 25-50 | 30-60 seconds |
| 3000-10000 chars | 50-150+ | 60-180 seconds |

**Factors affecting speed:**
- Number of concepts extracted
- Whether embeddings are generated
- Whether relationships are extracted
- OpenAI API response time
- Network latency

## Tips to Avoid Timeouts

### 1. **For Development/Testing**

Use higher thresholds to extract fewer concepts:
```typescript
body: JSON.stringify({
  text: inputText,
  min_importance: 0.5,  // Higher = fewer concepts = faster
  min_strength: 0.5,    // Higher = fewer edges = faster
  extract_relationships: true,
  generate_embeddings: true,
})
```

### 2. **For Production (Full Extraction)**

Use the defaults for unlimited extraction:
```typescript
body: JSON.stringify({
  text: inputText,
  min_importance: 0.0,  // 0.0 = unlimited (LLM decides)
  min_strength: 0.0,    // 0.0 = all edges
  extract_relationships: true,
  generate_embeddings: true,
})
```

### 3. **For Very Long Texts (>5000 chars)**

Consider:
- Splitting text into smaller sections
- Processing separately and combining graphs
- Or just wait 2-3 minutes - the system will handle it

## Monitoring Progress

Watch the backend console for real-time progress:

```
Processing 1 chunk(s) for 1234 characters
  Extracting concepts from chunk 1/1...
  Found 18 concepts
  Total concepts before deduplication: 18
  Generating embeddings and merging duplicates...
  Concepts after deduplication: 15
  Extracting relationships for 15 concepts...
  Found 32 relationships
Found 1 connected component(s)
Final graph has 1 connected component(s) - Target: 1
Added 0 inferred relationships for connectivity
INFO:     127.0.0.1:55235 - "POST /api/py/text/process HTTP/1.1" 200 OK
```

This helps you understand:
- How many concepts were found
- How many were duplicates
- How many relationships were extracted
- Whether the graph is fully connected

## Troubleshooting

### Still Getting Timeouts?

1. **Check OpenAI API rate limits**
   - Free tier has limits
   - Consider upgrading if hitting limits

2. **Reduce text length**
   - Start with 500-1000 characters
   - Gradually increase to test limits

3. **Skip embeddings temporarily**
   ```typescript
   generate_embeddings: false,  // Skips embedding generation (faster)
   ```

4. **Check backend is running**
   ```bash
   curl http://127.0.0.1:8000/api/py/health
   # Should return: {"status":"healthy","version":"0.1.0"}
   ```

5. **Check for errors in backend console**
   - Look for OpenAI API errors
   - Look for JSON parsing errors
   - Look for validation errors

### Frontend Shows Loading Forever?

Press F12 ‚Üí Network tab ‚Üí Check the request:
- **Pending**: Still processing (wait or cancel)
- **Failed**: Check error in Console tab
- **Timeout**: See error message on screen

### Backend Crashes?

Check console for:
- `OpenAI API key not found` ‚Üí Set `OPENAI_API_KEY` in `.env.local`
- `JSON parsing error` ‚Üí LLM returned invalid JSON (retry)
- `Out of memory` ‚Üí Text too long (reduce length)

## Performance Benchmarks

Tested on:
- **Hardware**: Standard laptop (8GB RAM)
- **Model**: gpt-4o-mini
- **Network**: Standard broadband

| Text Length | Concepts | Edges | Time |
|-------------|----------|-------|------|
| 500 chars | 8 | 12 | 12s |
| 1500 chars | 22 | 45 | 38s |
| 3000 chars | 45 | 98 | 76s |
| 5000 chars | 78 | 187 | 145s |

Your results may vary based on OpenAI API latency.

## What Changed

### Before
- ‚ùå Next.js proxy timeout at ~30 seconds (hardcoded)
- ‚ùå No progress indication
- ‚ùå Chunked all texts (slower)
- ‚ùå Generic error messages

### After
- ‚úÖ **Bypass Next.js proxy in development** (key fix!)
- ‚úÖ 3-minute timeout (configurable)
- ‚úÖ Progress logging in backend
- ‚úÖ Smart chunking (< 3000 chars = 1 chunk)
- ‚úÖ Specific timeout error messages
- ‚úÖ Extended keep-alive timeouts

## Future Improvements

Potential enhancements:
1. **Streaming responses** - Show concepts as they're extracted
2. **Progress bar** - Real-time UI progress indicator
3. **Caching** - Cache LLM responses for same text
4. **Background processing** - Use job queue for long texts
5. **Parallel processing** - Extract concepts from chunks in parallel

For now, the 3-minute timeout with progress logging should handle most use cases.

## Files Modified

- ‚úÖ `app/page.tsx` - **Bypass proxy + timeout handling (KEY FIX)**
- ‚úÖ `app/components/ChatPanel.tsx` - **Bypass proxy for Q&A endpoint**
- ‚úÖ `api/services/text_processing.py` - Optimization + logging
- ‚úÖ `start_backend.py` - Backend startup script with extended timeouts
- ‚úÖ `TIMEOUT_FIX.md` - Complete documentation

